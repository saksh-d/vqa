{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b9c1c0",
   "metadata": {},
   "source": [
    "# Visual Question Answering (VQA)\n",
    "This notebook implements a multi-modal architecture for VQA based on the DAQUAR dataset introduced here: [DAQUAR dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge/)\n",
    "\n",
    "The objective of this project is to merge two modalities: Images + Text and build an AI that can answer questions about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5b6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set up device agnostic code\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597d48a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The processed version of the dataset used in this project can be downloaded from Kaggle [here](https://www.kaggle.com/datasets/tezansahu/processed-daquar-dataset/data). Also contains the original dataset files.\n",
    "\n",
    "### Original dataset\n",
    "The original dataset has 3 files:\n",
    "- `all_qa_pairs.txt`\n",
    "- `train_images_list.txt`\n",
    "- `test_images_list.txt`\n",
    "\n",
    "### Processed dataset\n",
    "\n",
    "The processed version dataset contains a processed version of the full DAQUAR dataset, with the following descriptions:\n",
    "- `data.csv`: Processed dataset after normalizing all the questions and conversting the data into a tabular format {question, answer, image_id}\n",
    "- `data_train.csv`: Training data from `train_images_list.txt`\n",
    "- `data_eval.csv`: Testing data from `test_images_list.txt`\n",
    "- `answer_space.txt`: List of all possible answers extracted from `all_qa_pairs.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cc0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directory paths\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "IMG_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "TRAIN_CSV = DATA_DIR / \"data_train.csv\"\n",
    "EVAL_CSV = DATA_DIR / \"data_eval.csv\"\n",
    "ANS_TXT = DATA_DIR / \"answer_space.txt\"\n",
    "\n",
    "for p in [DATA_DIR, IMG_DIR, TRAIN_CSV, EVAL_CSV, ANS_TXT]:\n",
    "    assert p.exists(), f\"Missing: {p}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1c6428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6037e7036c0a4392a5ee9b407d249069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bad750b468748d58358bcee71b37651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700b3a25e0c424299d8d4bc29dfd4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d871c4e9394ab6af269a55a28b997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f64d4a5de5e40f385950e53ed3a41a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1154eda9e5104ce5ac25a1b1883a68eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'image_id', 'label', 'image_path'],\n",
      "        num_rows: 9974\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'image_id', 'label', 'image_path'],\n",
      "        num_rows: 2494\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CSVs + answer space and add labels\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": str(TRAIN_CSV), \"test\":str (EVAL_CSV)}\n",
    ")\n",
    "\n",
    "# answer space\n",
    "answers = [line.strip().lower() for line in ANS_TXT.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "ans_to_idx = {a: i for i, a in enumerate(answers)}\n",
    "\n",
    "if \"<unk>\" not in ans_to_idx:\n",
    "    ans_to_idx[\"<unk>\"] = len(ans_to_idx)\n",
    "    answers.append(\"<unk>\")\n",
    "\n",
    "def _norm(a: str) -> str:\n",
    "    return str(a).strip().lower()\n",
    "\n",
    "# take the first answer in case there are multiple answers\n",
    "def _to_label(a_raw: str) -> int: \n",
    "    first = _norm(a_raw.split(\",\")[0])\n",
    "    return ans_to_idx.get(first, ans_to_idx[\"<unk>\"])\n",
    "\n",
    "ds = ds.map(lambda b: {\"label\": [_to_label(a) for a in b[\"answer\"]]}, batched=True)\n",
    "\n",
    "def _resolve_path(image_id: str) -> str:\n",
    "    from os.path import exists\n",
    "    stem = image_id if image_id.endswith(\".png\") else f\"{image_id}.png\"\n",
    "    p = IMG_DIR / stem\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Not found: {p}\")\n",
    "    return str(p)\n",
    "\n",
    "ds = ds.map(lambda ex: {\"image_path\": _resolve_path(ex[\"image_id\"])})\n",
    "print(ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd8b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
